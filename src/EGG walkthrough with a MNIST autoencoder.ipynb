{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "EGG provides multiple components to create different types of games. In particular, EGG has strong tooling for creating one-step Sender/Receiver games. In these games, two agents, Sender and Receiver, are trained to perform a common task while following a simple communication protocol:\n",
    "\n",
    " * Sender receives an input (e.g., an image) and sends a single message to Receiver;\n",
    " * Receiver obtains the message and (optionally) its own input, produces an output.\n",
    "\n",
    "This type of games includes, for instance, signaling games and discrete auto-encoders. In a nutshell, any Sender/Receiver game only differs in terms of (a) input data, (b) agents' architecture, (c) communication type (one-symbol, fixed-lentgh or variable-length multiple-symbol messages), (d) loss. EGG allows us to create new games by only specifying those components. \n",
    "\n",
    "In most cases, training of agents with discrete channel communication is done either via Gumbel-Softmax relaxation or by means of Reinforce. EGG allows to switch between the two with minimal changes in the logic.\n",
    "\n",
    "\n",
    "\n",
    "## MNIST Auto-encoder game\n",
    "\n",
    "In this  tutorial we will create a conceptually simple, but fully-featured Sender/Receiver game. In this game, Sender and Receiver are trained together such that Sender tries to encode a MNIST digit in a discrete message and Receiver will try to decode it from the message.\n",
    "\n",
    "While implementing this game, we will walk through several steps, typical for creating a new game:\n",
    " * pre-train a _vision_ module for the Sender agent by training it to classify digits;\n",
    " * use the pre-trained vision module to implement Sender and Receiver that communicate via a single symbol messages and analyse the trained models;\n",
    " * update the agents to allow variable-length multi-symbol messages.\n",
    "\n",
    "NB: depending on the computational resources you have available, this might be slow to run. To get at least a sense of the full pipeline, consider reducing the number of training epochs, or, alternatively, run a Google Colab version of the tutorial using a GPU.\n",
    "\n",
    "So let's start!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      2\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mload_ext\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mautoreload\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      3\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mautoreload\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01megg\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcore\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import egg.core as core\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 5, 10\n",
    "\n",
    "# For convenince and reproducibility, we set some EGG-level command line arguments here\n",
    "opts = core.init(params=['--random_seed=7', # will initialize numpy, torch, and python RNGs\n",
    "                         '--lr=1e-3',   # sets the learning rate for the selected optimizer \n",
    "                         '--batch_size=32',\n",
    "                         '--optimizer=adam'])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most cases, we want to start a script by initializing EGG by calling `core.init` Typically, we use command-line arguments for that (see https://github.com/fairinternal/EGG/blob/master/egg/core/CL.md for more details)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement our ```Vision``` module (a part of a standard MNIST [model](https://github.com/pytorch/examples/blob/master/mnist/main.py)) that maps a MNIST image into a 500-dimensional vector.\n",
    "To pre-train this ```Vision``` module, we'll use the auxilary task of classifying MNIST digits. For this, we also define a ```PretrainNet``` model that takes the output of ```Vision``` and classifies images into 10 classes (0, 1, ..., 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vision(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vision, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n",
    "\n",
    "class PretrainNet(nn.Module):\n",
    "    def __init__(self, vision_module):\n",
    "        super(PretrainNet, self).__init__()\n",
    "        self.vision_module = vision_module\n",
    "        self.fc = nn.Linear(500, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.vision_module(x)\n",
    "        x = self.fc(F.leaky_relu(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need DataLoaders for MNIST...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 1, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch_size = opts.batch_size # set via the CL arguments above\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=True, download=True,\n",
    "           transform=transform),\n",
    "           batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('./data', train=False, transform=transform),\n",
    "           batch_size=batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, everything is ready to actually run the pre-training. First, we instantiate the modules and then run 10 epochs of MNIST digit recognition by a PretrainNet instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = Vision()\n",
    "class_prediction = PretrainNet(vision) #  note that we pass vision - which we want to pretrain\n",
    "optimizer = core.build_optimizer(class_prediction.parameters()) #  uses command-line parameters we passed to core.init\n",
    "class_prediction = class_prediction.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run very typical Pytorch training loop to pretrain the vision module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m output = class_prediction(data)\n\u001b[32m      7\u001b[39m loss = F.cross_entropy(output, target)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m optimizer.step()\n\u001b[32m     11\u001b[39m mean_loss += loss.mean().item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/25j9pl8s9g13dkmn7hfrbw8h06yagzmz-devenv-profile/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/25j9pl8s9g13dkmn7hfrbw8h06yagzmz-devenv-profile/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/nix/store/25j9pl8s9g13dkmn7hfrbw8h06yagzmz-devenv-profile/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    mean_loss, n_batches = 0, 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = class_prediction(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        mean_loss += loss.mean().item()\n",
    "        n_batches += 1\n",
    "        \n",
    "    print(f'Train Epoch: {epoch}, mean loss: {mean_loss / n_batches}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! Now ```vision``` is a pre-trained ```Vision``` module and we can proceed to actually implement the communication game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-symbol communication\n",
    "\n",
    "As said before, Sender/Receiver games only differ in the data used, the architecture of the agents, the type of the channel, and the loss.\n",
    "We will start with the simplest communication case, one-symbol communication.\n",
    "\n",
    "\n",
    "Let's define our Sender (that will re-use the pre-trained ```vision``` module) and Receiver. \n",
    "For implementing single-symbol games, EGG provides special wrappers, ```GumbelSoftmaxWrapper``` and ```ReinforceWrapper``` that wrap an agent and implement the required Gumbel-Softmax or Reinforce-specific logic. Both wrappers assume that the agent returns log-probabilities over all terms for the vocabulary.\n",
    "\n",
    "We start by defining the logic of our agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sender(nn.Module):\n",
    "    def __init__(self, vision, output_size):\n",
    "        super(Sender, self).__init__()\n",
    "        self.fc = nn.Linear(500, output_size)\n",
    "        self.vision = vision\n",
    "        \n",
    "    def forward(self, x, aux_input=None):\n",
    "        with torch.no_grad():\n",
    "            x = self.vision(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Receiver(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Receiver, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, 784)\n",
    "\n",
    "    def forward(self, channel_input, receiver_input=None, aux_input=None):\n",
    "        x = self.fc(channel_input)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "sender = Sender(vision, output_size=400)\n",
    "receiver = Receiver(input_size=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While ```GumbelSoftmaxWrapper``` and ```ReinforceWrapper``` have the same interface to the user's defined Sender and Receiver, they do differ in their output.\n",
    "\n",
    "Since ```GumbelSoftmaxWrapper``` uses relaxation of the symbols, it doesn't output the index of the used symbol; instead, it outputs a tensor of dimensions (batch size; vocab size) with potentially all elements being non-zero. In evaluation mode, it outputs a tensor of the same dimensionality, but with the one-hot encoded symbol of highest probability. In contrast, ```ReinforceWrapper``` directly outputs the index of the transmitted symbol (plus other data to be discussed below). \n",
    "\n",
    "To make the change of wrappers simpler, we define a special helper layer, `core.RelaxedEmbedding` which behaves as a standard `torch.nn.Embedding` layer if the inputs are indexes (i.e. are of type torch.long), or provides their relaxation, if they are a tensor of floats. You don't have to use this, but it makes changing the training from GS-based to RF-based and vice-versa very simple, because you do not need to worry about the type of the input that the Receiver will get.\n",
    "\n",
    "Furthermore, you can also wrap your Receiver agent logic in the `core.SymbolReceiverWrapper`. This wrapper would embed the input symbol via the `core.RelaxedEmbedding` and pass the resulting embedding to the user-implement agent.\n",
    "\n",
    "(NB: `core.RelaxedEmbedding` has the same initialization as `torch.nn.Embedding` - different from that of `nn.Linear`!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from specifying the agents' architecture, we have to define the loss that the agents experience. By default, EGG expects the loss function to take the following arguments: Sender's input, the message transmitted, the input of the receiver, the output of the receiver, and the labels provided from the dataset. \n",
    "Any of those can be used or ignored. In the case of our MNIST auto-encoder game, there is no specific input for Receiver, so the game mechanics passes `None` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(sender_input, _message, _receiver_input, receiver_output, _labels, _aux_input=None):\n",
    "    loss = F.binary_cross_entropy(receiver_output, sender_input.view(-1, 784), reduction='none').mean(dim=1)\n",
    "    return loss, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this loss function is differentiable; it doesn't have to be differentiable if the training is done via Reinforce. However, if it is the case, the training of Receiver might be able to exploit that. See the [readme](https://github.com/facebookresearch/EGG/blob/master/README.md#an-important-technical-point) for more discussion.\n",
    "\n",
    "Another common parameter that would be shared across all the games we'll implement is the size of the vocabulary. We set it to 10 as MNIST has 10 digits; although it might be fun to play with other values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Gumbel-Softmax\n",
    "\n",
    "The next step is to actually create the agents. We first discuss how training can be done with Gumbel-Softmax. For that, we instantiate our Sender and Receiver and wrap Sender into `core.GumbelSoftmaxWrapper`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(vision, vocab_size)\n",
    "sender = core.GumbelSoftmaxWrapper(sender, temperature=1.0) # wrapping into a GS interface, requires GS temperature\n",
    "receiver = Receiver(input_size=400)\n",
    "receiver = core.SymbolReceiverWrapper(receiver, vocab_size, agent_input_size=400)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the agents created, we can adopt a pre-defined game logic which juggles the agents, messages, \n",
    "data, and the loss. Essentially, the game instance is also a `torch.nn.Module` - hence it can be handled as that, i.e.\n",
    "we can extract the trainable parameters of the agents, push it around devices, etc. Each game is expected to output a two-tuple. The first element of the tuple is a loss that is to be minimized. The second is a python dict `{}` of some auxilary values that are averaged over the epoch and printed. Accuracy or entropy of the communication channel are good examples of such auxilary data.\n",
    "\n",
    "\n",
    "Another entity that we will need is `core.Trainer`. Essentially, `Trainer` implements the training loop, handles checkpointing, etc.\n",
    "\n",
    "\n",
    "The Gumbel-Softmax distribution is parameterised by its temperature. In general, the latter has strong impact on training. To anneal the temperature of Sender, we create a callback that `trainer` invokes after each training epoch is over. In this snippet, the callback applies a multiplicative decay rule on the temperature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = core.SymbolGameGS(sender, receiver, loss)\n",
    "optimizer = torch.optim.Adam(game.parameters())\n",
    "\n",
    "trainer = core.Trainer(\n",
    "    game=game, optimizer=optimizer, train_data=train_loader,\n",
    "    validation_data=test_loader, callbacks=[core.TemperatureUpdater(agent=sender, decay=0.9, minimum=0.1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay, everything is ready for actual training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "trainer.train(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when Sender and Receiver are trained, we firstly have a look at the codebook they came up with: what does Receiver output when given a particular word from a vocabulary? For that, we'll take every word in the vocabulary [0..9], wrap it into a tensor and pass it to Receiver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.eval()\n",
    "\n",
    "for z in range(vocab_size):\n",
    "    t = torch.zeros(vocab_size).to(device)\n",
    "    t[z] = 1\n",
    "    with torch.no_grad():\n",
    "        # Receiver outputs a single tensor of predictions\n",
    "        sample = game.receiver(t).float().cpu()\n",
    "    sample = sample.view(28, 28)\n",
    "    plt.title(f\"Input: symbol {z}\")\n",
    "    plt.imshow(sample, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, not too bad: some digits are vague, but others do resemble actual digits. Huh!\n",
    "\n",
    "\n",
    "Next, how good are our two agents at auto-encoding the images? Let's take some images from the validation set, feed them to Sender and look at (a) what messages are sent, (b) how does Receiver's output look like?\n",
    "\n",
    "First, we define a tiny, single-batch 10-digit test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = []\n",
    "for z in range(10):\n",
    "    index = (test_loader.dataset.targets[:100] == z).nonzero()[0, 0]\n",
    "    img, _ = test_loader.dataset[index]\n",
    "    test_inputs.append(img.unsqueeze(0))\n",
    "test_inputs = torch.cat(test_inputs)\n",
    "\n",
    "test_dataset = [[test_inputs, None]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a small helper function to dump and plot the input-outputs of the agents along with the communicated messages. We will use it throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(game, test_dataset, is_gs, variable_length):\n",
    "    interaction = \\\n",
    "            core.dump_interactions(game, test_dataset, is_gs, variable_length)\n",
    "\n",
    "    for z in range(10):\n",
    "        src = interaction.sender_input[z].squeeze(0)\n",
    "        dst = interaction.receiver_output[z].view(28, 28)\n",
    "        # we'll plot two images side-by-side: the original (left) and the reconstruction\n",
    "        image = torch.cat([src, dst], dim=1).cpu().numpy()\n",
    "\n",
    "        plt.title(f\"Input: digit {z}, channel message {interaction.message[z]}\")\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the left image is the original and the one on the right is the recovered one, each pair is labelled by the original class and the symbol sent over the channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(game, test_dataset, is_gs=True, variable_length=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, not totally unreasonable! \n",
    "\n",
    "### Training with Reinforce\n",
    "What's the equivalent code when training by Reinforce?\n",
    "\n",
    "We create exactly the same Sender instance, but then feed it into ```core.ReinforceWrapper```. This wrapper implements the boilerplate code required for Reinforce-based optimization: samples from a distribution, records the log-probability of what was sampled, and reports the entropy of the distribution (*).\n",
    "\n",
    "Hence, each agent returns a tuple of three elements: the actual sampled output, the log-probability of the sampled output, and the entropy of the sampling distribution. During the evaluation, the sampling process is replaced by taking the most likely output.\n",
    "\n",
    "But what if an agent (e.g. Receiver) runs a deterministic function? In that case, we use a simple `core.ReinforceDeterministicWrapper` that appends zero entropy and log-probability tensors. `ReinforceDeterministicWrapper` can only be used if Receiver's parameters can be optimized via a gradient-based optimization, i.e. the loss function has to be differentiable.\n",
    "\n",
    "\n",
    "\n",
    "(*) Entropy regularization is used to encourage the agents to do some exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(vision, output_size=vocab_size)\n",
    "sender = core.ReinforceWrapper(sender)  # wrapping into a Reinforce interface\n",
    "\n",
    "receiver = Receiver(input_size=400)\n",
    "receiver = core.SymbolReceiverWrapper(receiver, vocab_size, agent_input_size=400)\n",
    "receiver = core.ReinforceDeterministicWrapper(receiver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we use the one-symbol game implementation with Reinforce logic, `core.SymbolGameReinforce`. It also consumes two entropy regularization terms, for Sender and Receiver. The entropies of the output distributions of the agents are weighted by these parameters and added to the loss.\n",
    "\n",
    "Note that we managed to change from Gumbel Softmax to Reinforce training by only (a) changing the agent wrappers, (b) changing the game type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = core.SymbolGameReinforce(sender, receiver, loss, sender_entropy_coeff=0.05, receiver_entropy_coeff=0.0)\n",
    "optimizer = torch.optim.Adam(game.parameters(), lr=1e-2) #  we can also use a manually set optimizer\n",
    "\n",
    "trainer = core.Trainer(game=game, optimizer=optimizer, train_data=train_loader,\n",
    "                           validation_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 15\n",
    "trainer.train(n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check what is in the communication code book and the quality of the auto-encoding. The dumping code is essentially the same as before, the only change is that Receiver returns a tuple of three elements, and we need only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.eval()\n",
    "\n",
    "for z in range(vocab_size):\n",
    "    t = torch.zeros(vocab_size).to(device)\n",
    "    t[z] = 1\n",
    "    with torch.no_grad():\n",
    "        sample, _1, _2 = game.receiver(t)\n",
    "        sample = sample.float().cpu()\n",
    "    sample = sample.view(28, 28)\n",
    "    plt.title(f\"Input: symbol {z}\")\n",
    "    plt.imshow(sample, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(game, test_dataset, is_gs=False, variable_length=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad either! Let us move to a more interesting case: variable-length messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Communication with variable-length messages\n",
    "\n",
    "This time, communication is carried out by RNNs. EGG provides pre-implemented code to use vanilla RNNs, GRUs, or LSTMs. There is also an experimental support for communication via Transformer-based blocks.\n",
    "\n",
    " Note that, when communicating over variable-length sequences, we need to define a special end-of-sequence symbol. The agents are trained such that the loss is calculated wrt to the Receiver output either after receiving the end-of-sequence symbol or after the maximal number of symbols were produced. EGG handles those scenarios, by defining special wrappers for agents, RnnSenderGS and RnnReceiverGS for Gumbel Softmax-based learning, and RnnSenderReinforce and RnnReceiverReinforce for Reinforce-based learning.\n",
    "\n",
    "By convention, the user-implemented Sender, after processing its input, outputs the initial hidden state vector for its RNN. EGG unrolls this RNN to produce a message. In turn, this message is fed into Receiver's RNN. The only thing that we need to implement for Receiver is how we map the Receiver RNN's hidden state to the Receiver's output.\n",
    "\n",
    "**Important points concerning end-of-sequence behaviour: (1) EGG assumes that the end-of-sequence symbol is always 0. (2) `vocab_size` always includes the end-of-sequence symbol - hence `vocab_size=2` specifies the unary encoding. (3) Communication stops immediately if the first symbol emitted by Sender is 0.**\n",
    "\n",
    "This time we'll have to specify some additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we start by defining the RNN parameters\n",
    "hidden_size = 20\n",
    "emb_size = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the same pre-trained vision module. Note, this time **Sender does not output a vector which will be casted as logits of probabilities over the vocab - it only outputs the initial hidden state for its RNN cell**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is pretty much the same as before: we instantiate the instances and put them into wrappers (note that we set the RNN cell types to 'rnn' and maximal communication length `max_len=2`!)\n",
    "\n",
    "First, we'll go through Gumbel Softmax-based training.\n",
    "\n",
    "\n",
    "### Variable-length messages with Gumbel Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(vision, hidden_size)\n",
    "receiver = Receiver(hidden_size)\n",
    "\n",
    "sender_rnn = core.RnnSenderGS(sender, vocab_size, emb_size, hidden_size,\n",
    "                                   cell=\"gru\", max_len=2, temperature=1.0)\n",
    "receiver_rnn = core.RnnReceiverGS(receiver, vocab_size, emb_size,\n",
    "                    hidden_size, cell=\"gru\")\n",
    "\n",
    "game_rnn = core.SenderReceiverRnnGS(sender_rnn, receiver_rnn, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that sometimes specifying different learning speeds for the two agents aids learning; this can be done as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "        {'params': game_rnn.sender.parameters(), 'lr': 1e-3},\n",
    "        {'params': game_rnn.receiver.parameters(), 'lr': 1e-2}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = core.Trainer(game=game_rnn, optimizer=optimizer, train_data=train_loader,\n",
    "                           validation_data=test_loader)\n",
    "trainer.train(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heh, the validation loss is lower than it used to be with one symbol communication. Let's see how it works. Again, the original image is on the left, the transmitted and decoded - on the right. The communicated message is in the title. Note that since default `vocab_size` is 10 and 0 is used for end-of-sequence, Sender has an effective vocabulary of 9 symbols.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(game_rnn, test_dataset, is_gs=True, variable_length=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work better than with length-one!\n",
    " \n",
    "We can also enumerate all possible 10x10 messages of length two {x = first symbol, y = second symbol} and check the Receiver's interpretation of the code, as shown next. NB: When x=0, the second symbol is ignored, as 0 is interpreted as end-of-sequence. For this reason, in this case Receiver generates the same image irrespective of y's value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(10, 10, sharex=True, sharey=True)\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(10):\n",
    "            \n",
    "        t = torch.zeros((1, 2, vocab_size)).to(device)\n",
    "        t[0, 0, x] = 1\n",
    "        t[0, 1, y] = 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample = game_rnn.receiver(t).float().cpu()\n",
    "            # 0 is the end-of-sequence symbol, hence we stop immediately when x == 0\n",
    "            output_index = (0 if x == 0 else 1)\n",
    "            sample = sample[0, output_index, :].view(28, 28)\n",
    "            ax[x][y].imshow(sample, cmap='gray')\n",
    "            \n",
    "            if y == 0:\n",
    "                ax[x][y].set_ylabel(f'x={x}')\n",
    "            if x == 0:\n",
    "                ax[x][y].set_title(f'y={y}')\n",
    "            \n",
    "            ax[x][y].set_yticklabels([])\n",
    "            ax[x][y].set_xticklabels([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness, we re-iterate by training with Reinforce.\n",
    "\n",
    "### Variable-length messages with Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sender = Sender(vision, hidden_size)\n",
    "receiver = Receiver(hidden_size)\n",
    "\n",
    "sender_rnn = core.RnnSenderReinforce(sender, vocab_size, emb_size, hidden_size,\n",
    "                                   cell=\"gru\", max_len=2)\n",
    "receiver_rnn = core.RnnReceiverDeterministic(receiver, vocab_size, emb_size,\n",
    "                    hidden_size, cell=\"gru\")\n",
    "\n",
    "game_rnn = core.SenderReceiverRnnReinforce(sender_rnn, receiver_rnn, loss, \n",
    "                                           sender_entropy_coeff=0.015, \n",
    "                                           receiver_entropy_coeff=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam([\n",
    "        {'params': game_rnn.sender.parameters(), 'lr': 1e-3},\n",
    "        {'params': game_rnn.receiver.parameters(), 'lr': 1e-2}\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = core.Trainer(game=game_rnn, optimizer=optimizer, train_data=train_loader,\n",
    "                           validation_data=test_loader)\n",
    "trainer.train(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check how the auto-encoding is done and the emerging protocol.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(game_rnn, test_dataset, is_gs=False, variable_length=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(10, 10, sharex=True, sharey=True)\n",
    "\n",
    "for x in range(10):\n",
    "    for y in range(10):\n",
    "            \n",
    "        t = torch.zeros((1, 2)).to(device).long()\n",
    "        t[0, 0] = x\n",
    "        t[0, 1] = y\n",
    "\n",
    "        with torch.no_grad():\n",
    "            sample = game_rnn.receiver(t)[0].float().cpu()\n",
    "            sample = sample[0, :].view(28, 28)\n",
    "            ax[x][y].imshow(sample, cmap='gray')\n",
    "            \n",
    "            if y == 0:\n",
    "                ax[x][y].set_ylabel(f'x={x}')\n",
    "            if x == 0:\n",
    "                ax[x][y].set_title(f'y={y}')\n",
    "            \n",
    "            ax[x][y].set_yticklabels([])\n",
    "            ax[x][y].set_xticklabels([])\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the results are different. \n",
    "Sometimes (eg `x=3`, `x=7`) it looks like the first symbol in the message (x) encodes which digit Receiver should reproduce, while the second (y) has more effect on the style of the digit. But is it true? How would we check that? 🤔\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (advanced under the hood stuff, may be skipped): optimization with stochastic nodes via Reinforce\n",
    "\n",
    "When we apply the differentiable relaxation of the communication channel by replacing the discrete messages with their Gumbel Softmax approximations, we make the entire Sender->Receiver pipeline differentiable. In contrast, Reinforce-based training doesn't make any relaxations. Instead, it effectively \"smoothes\" the channel by operating with a randomized distribution of messages, parameterised by Sender's output distribution.\n",
    "\n",
    "However, things might get more tricky with Reinforce-based training. In particular, it can happen that the loss on the Receiver's side is differentiable (as in our MNIST example above), and, in this case, we can train Receiver using the standard backprop. It might also happen that the loss is not differentiable (imagine 0-1 accuracy loss). In the latter case, Receiver also has to randomise its outputs.\n",
    "\n",
    "*EGG handles both cases with the same API*, however, it can be useful to know how it operates internally.\n",
    "\n",
    "Now, let's have a closer look at the optimisation problem. By $\\theta_s$ and $\\theta_r$ we denote the parameters of Sender and Receiver, respectively; $m$ is the message sent, and it comes from Sender's output distribution $P(m | i, \\theta_s)$; $R(m, \\theta_r)$ is Receiver's output, and $i$ is Sender's input. \n",
    "\n",
    "### Deterministic Receiver\n",
    "When Receiver is deterministic, we want to minimize the following aggregate loss for an input $i$:\n",
    "\n",
    "$$L = \\mathbb{E}_{m \\sim P(m | i, \\theta_s)}~ l(R(m, \\theta_r), m, i)$$\n",
    "\n",
    "The gradient wrt to ${\\theta_r}$ would be:\n",
    "\n",
    "$$\\nabla_{\\theta_r} L = \\nabla_{\\theta_r}\\mathbb{E}_{m \\sim P(m | i, \\theta_s)}~ l(R(m, \\theta_r), m, i) = \\mathbb{E}_{m \\sim P(m | i, \\theta_s)}~ \\nabla_{\\theta_r} l(R(m, \\theta_r), m, i) = \\mathbb{E}_{m \\sim P(m | i, \\theta_s)} \\frac{\\partial}{\\partial r } l(r, m, i) \\cdot \\nabla_{\\theta_r} R(m, \\theta_r)\n",
    "$$\n",
    "\n",
    "We can estimate it by sampling $N$ messages from Sender:\n",
    "$$\\frac{1}{N} \\sum_m \\frac{\\partial}{\\partial r } l(r, m, i) \\cdot \\nabla_{\\theta_r} R(m, \\theta_r)\n",
    "$$\n",
    "Note that this gradient of $\\theta_r$ is obtained by standard back-propogation from the loss with the messages sampled from Sender. By using the Reinforce trick, we estimate the second gradient by sampling $N$ messages from $P(m | i, \\theta_s)$:\n",
    "\n",
    "$$\\nabla_{\\theta_s} L = \\frac{1}{N} \\sum_m l(R(m, \\theta_r), m, i) \\nabla_{\\theta_s} \\log P(m | i, \\theta_s) $$\n",
    "\n",
    "**In this case, in Pytorch the gradients of both agents can be obtained by differentiating the sum**\n",
    "\n",
    "```(l.mean() + (l.detach() * log_prob_sender).mean()).backwards()```\n",
    "\n",
    "(note: EGG also utilizes the mean baseline to reduce variance of the gradient estimate, but we omit that here for simplicity).\n",
    "\n",
    "\n",
    "### Stochastic Receiver\n",
    "When the loss $l$ is not differentiable, and we sample output $o$ from Receiver $o \\sim P(o | m, \\theta_r)$, the expected loss becomes:\n",
    "\n",
    "$$L = \\mathbb{E}_{m, o \\sim P(m, o | i, \\theta_s, \\theta_r)} ~ l(o, m, i)$$\n",
    "\n",
    "The gradient wrt $\\theta_r$ would be\n",
    "$$\\nabla_{\\theta_r} L = \\mathbb{E}_{m, o \\sim P(m, o | i, \\theta_s, \\theta_r)} l(o, m, i) ~ \\nabla_{\\theta_r} log P(m, o | i, \\theta_s, \\theta_r) = \\mathbb{E}_{m, o \\sim P(m, o | i, \\theta_s, \\theta_r)} l(o, m, i) ~ \\nabla_{\\theta_r} log P(o | m, \\theta_r)$$\n",
    "\n",
    "By sampling $N$ $(m, o)$ pairs, we get\n",
    "$$\\nabla_{\\theta_r} L \\approx \\frac{1}{N} \\sum_{o, m} l(o, m, i) ~ \\nabla_{\\theta_r} log P(o | m, \\theta_r)$$\n",
    "\n",
    "and similarly, \n",
    "$$\\nabla_{\\theta_s} L \\approx \\frac{1}{N} \\sum_{o, m} l(o, m, i) ~ \\nabla_{\\theta_s} log P(m | i, \\theta_s)$$\n",
    "\n",
    "\n",
    "**In this case, in Pytorch the gradients of both agents can be obtained by differentiating the sum**\n",
    "\n",
    "```(l.detach() * (log_prob_sender + log_prob_receiver).mean()).backwards()```\n",
    "\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "Now, we can notice that if `l` is not differentiable, the gradient of `l.mean()` would be zero. Conversely, if Receiver is deterministic, `log_prob_receiver` would be the zero constant, hence its gradient is zero, too.\n",
    "\n",
    "Thus, both cases can be covered by minimizing the loss:\n",
    "\n",
    "```(l.mean() + (l.detach() * (log_prob_sender + log_prob_receiver).mean())).backwards()```\n",
    "\n",
    "And that is what EGG does internally. Depending on whether Receiver is wrapped in `ReinforceDeterministicWrapper`/`RnnReceiverDeterministic` or in `ReinforceWrapper`/`RnnReceiverReinforce` defines which part of the loss would be effective for training Receiver. \n",
    "\n",
    "\n",
    "Overall, this makes it hard to implement a general Reinforce interface for all possible losses. Some involved cases might require re-implementing game mechanics or resorting to Gumbel Softmax relaxation, which doesn't have this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
